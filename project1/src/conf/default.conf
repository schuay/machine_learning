# A section is built as follows:
#
# [classifier_instance_name]
# type = the_classifier_type
# option = value
# [...]
#
# Common options are:
#
# training_subset_ratio: float. The probability of using a presented training
# instance (others are ignored).
#
# training_subset_seed: int. The seed used for determining whether to use a
# training instance.
#
# Options for naive bayes as in:
# http://www.nltk.org/_modules/nltk/classify/naivebayes.html
#
# KNN:
# http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
#
# SVM:
# http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html
#
# Decision tree:
# http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
# 
# Extra tree:
# http://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html
#
# Float and integer options must be added to OPTION_CONVERSIONS in ensemble.py.
#
# The special "ensembles" section must always be present. The ensembles used are specified
# in this section. The option name is the name of the ensemble and the value is a comma
# separated list of classifier section names.

[ensembles]
mix_default = nb0, knn0, svm0, tree0, extratree0
nb7 = nb1, nb2, nb3, nb4, nb5, nb6, nb7
knn7 = knn1, knn2, knn3, knn5, knn7, knn8, knn9
svm7 = svm01, svm05, svm10, svm15, svm20, svm25, svm30
tree7 = extratree1, extratree2, extratree3, treebestgini, treerandomgini, treebestentropy, treerandomentropy
extraforest = t0, t1, t2, t3, t4, t5, t6, t7, t8, t9

[nb0]
kind = naive_bayes

[knn0]
kind = knn

[svm0]
kind = svm

[tree0]
kind = tree

[extratree0]
kind = extra_tree

[nb1]
kind = naive_bayes
training_subset_ratio = 0.75
training_subset_seed  = 1

[nb2]
kind = naive_bayes
training_subset_ratio = 0.75
training_subset_seed  = 2

[nb3]
kind = naive_bayes
training_subset_ratio = 0.75
training_subset_seed  = 3

[nb4]
kind = naive_bayes
training_subset_ratio = 0.75
training_subset_seed  = 4

[nb5]
kind = naive_bayes
training_subset_ratio = 0.75
training_subset_seed  = 5

[nb6]
kind = naive_bayes
training_subset_ratio = 0.75
training_subset_seed  = 6

[nb7]
kind = naive_bayes
training_subset_ratio = 0.75
training_subset_seed  = 7

[knn1]
kind = knn
n_neighbors = 1

[knn2]
kind = knn
n_neighbors = 2

[knn3]
kind = knn
n_neighbors = 3

[knn5]
kind = knn
n_neighbors = 5

[knn7]
kind = knn
n_neighbors = 7

[knn8]
kind = knn
n_neighbors = 8

[knn9]
kind = knn
n_neighbors = 9

[svm01]
kind = svm
C = 0.1

[svm05]
kind = svm
C = 0.5

[svm10]
kind = svm
C = 1.0

[svm15]
kind = svm
C = 1.5

[svm20]
kind = svm
C = 2.0

[svm25]
kind = svm
C = 2.5

[svm30]
kind = svm
C = 3.0

[extratree1]
kind = extra_tree

[extratree2]
kind = extra_tree

[extratree3]
kind = extra_tree

[treebestgini]
kind = tree
criterion = gini
splitter = best

[treerandomgini]
kind = tree
criterion = gini
splitter = random

[treebestentropy]
kind = tree
criterion = entropy
splitter = best

[treerandomentropy]
kind = tree
criterion = entropy
splitter = random

[t0]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 0

[t1]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 1

[t2]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 2

[t3]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 3

[t4]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 4

[t5]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 5

[t6]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 6

[t7]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 7

[t8]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 8

[t9]
kind = extra_tree
training_subset_ratio = 0.75
training_subset_seed  = 9
