\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{bashful}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{xspace}

\lstset{
    language=Python,
    basicstyle=\ttfamily,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\lstdefinestyle{BashInputStyle}{
  language=bash,
  firstline=2,% Supress the first line that begins with `%`
  basicstyle=\small\sffamily,
  numbers=left,
  numberstyle=\tiny,
  numbersep=3pt,
  frame=tb,
  columns=fullflexible,
  backgroundcolor=\color{yellow!20},
  linewidth=0.9\linewidth,
  xleftmargin=0.1\linewidth
}

\lstdefinestyle{BashOutputStyle}{
  basicstyle=\small\ttfamily,
  numbers=none,
  frame=tblr,
  columns=fullflexible,
  backgroundcolor=\color{gray!10},
}

\title{Exercise 2 \\
       Machine Learning WS 2014/2015 \\
       Technical University of Vienna}
\author{Jakob Gruber, 0203440 \\
        Mino Sharkhawy, 1025887}

\newcommand{\breastcancer}{\texttt{breast-cancer\_scale}\xspace}
\newcommand{\vehicle}{\texttt{vehicle.scale}\xspace}
\newcommand{\randomtwo}{\texttt{random2}\xspace}
\newcommand{\hastie}{\texttt{hastie}\xspace}
\newcommand{\digits}{\texttt{digits}\xspace}
\newcommand{\iris}{\texttt{iris}\xspace}
\newcommand{\heart}{\texttt{heart\_scale}\xspace}
\newcommand{\connectfour}{\texttt{connect-4}\xspace}
\newcommand{\vowel}{\texttt{vowel.scale}\xspace}
\newcommand{\annealing}{\texttt{annealing}\xspace}
\newcommand{\randomten}{\texttt{random10}\xspace}

\newcommand{\simplemaj}{\texttt{simple\_majority}\xspace}
\newcommand{\weightedmaj}{\texttt{weighted\_majority}\xspace}

\begin{document}

\maketitle

\newpage
\tableofcontents

% --------------------------------------------------------------------------------------------------

\newpage
\section{Introduction}

This assignment consists of:

\begin{itemize}
\item collecting a suite of different datasets,
\item implementing two ensemble learning strategies: simple majority and weighted majority voting,
\item evaluating base- and ensemble classifier effectiveness and efficiency on this suite,
\item and finally providing a simple user interface.
\end{itemize}

% --------------------------------------------------------------------------------------------------

\section{Materials and Methods}

% --------------------------------------------------------------------------------------------------

\subsection{Dataset Selection}

%TODO

% --------------------------------------------------------------------------------------------------

\subsection{Libraries, Toolkits, etc.}

We based our experiments on the machine learning toolkit
\verb|scikit-learn 0.15.2|\footnote{\url{http://scikit-learn.org/stable/}}.

Some of the datasets used (\verb|breast-cancer_scale|, \verb|heart_scale|,
\verb|connect-4|, \verb|vehicle.scale| and \verb|vowel.scale|) are part of
\verb|libSVM|\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}}.

The entire application logic
(loading, preprocessing, training, and evaluation) is written in Python, giving us
maximal flexibility.

Results were exported to CSV files, which were then used to generate graphs
using R and the \verb|ggplot2|\footnote{\url{http://ggplot2.org/}} package.

Obviously, this report was typeset in \LaTeX\space and (less obviously) integrated with
R using \verb|sweave|\footnote{\url{https://www.stat.uni-muenchen.de/~leisch/Sweave/}}.

% --------------------------------------------------------------------------------------------------

\subsection{Classifier Selection}

In this assignment, we used \verb|scikit-learn| exclusively. We selected the
following base classifiers:

\begin{itemize}
\item \lstinline|KNeighborsClassifier|: A k-nearest neighbors classifier with
      a configurable number of neighbors and distance metric.
\item \lstinline|GaussianNB|: A simple gaussian naive bayes classifier.
\item \lstinline|LinearSVC|: A linear support vector machine classifier with
      configurable penalty parameters.
\item \lstinline|DecisionTreeClassifier|: A decision tree with configurable split
      quality criteria.
\item \lstinline|ExtraTreeClassifier|: An extremely randomized tree classifier,
      intended especially for ensemble classifiers. Random splits are drawn
      for a number of features at each node, and the best among these is chosen.
\end{itemize}

These base classifiers were then combined into ensemble classifiers in various
configurations. We experimented both with including classifiers of different configurations,
and training several identical classifiers on a random sample of the training set (i.e., bagging).

Ensemble classifiers were implemented in two variations:

\begin{itemize}
\item The \simplemaj ensemble classifier uses a simple majority vote to determine
      the winning class, while
\item the \weightedmaj ensemble classifier uses predetermined accuracies of each
      base classifier to distribute weight according to a classifier's effectiveness.
\end{itemize}

% --------------------------------------------------------------------------------------------------

\subsection{Toolchain}

The \lstinline|data| directory contains subdirectories for the used datasets that are not
part of \verb|scikit-learn|. By executing \lstinline|make| in \lstinline|data| all of
these datasets are downloaded to their respective directories. Each dataset is represented
by a class in the \lstinline|src| directory. The datasets from \verb|libSVM| share a
single directory in \lstinline|data|.

The ensembles are specified via an INI-style configuration file. Each section header
contains a classifier name. The Python class which provides the underlying algorithm is
specified with the \texttt{kind} option. Classifiers trained on random samples of
the training set are configured using the \texttt{training\_subset\_ratio} and
\texttt{training\_subset\_seed} options. All other options in a section specify options
specific to this class. The special \texttt{ensemble} section is used to specify
ensembles. Here, each option is the name of an ensemble and the option's value is a comma
separated list of classifier names.

Config files reside in \lstinline|src/conf|. Listing~\ref{lst:config} shows one of the
many available sample configs. This one defines an ensemble of three KNN-classifiers,
each of which is configured to use a different number of neighbors and train on
a random sample $75\%$ the size of the training set.

\lstinputlisting[label=lst:config,style=BashOutputStyle,caption=src/conf/knn3.conf]{../src/conf/knn3.conf}

When \lstinline|ensemble.py| in \lstinline|src| is called, it evaluates the performance of
all classifiers in the config file and of the simple and weighted majority ensemble
classifers on the specified dataset and prints the results in the CSV format to the
terminal.

Listing~\ref{lst:ensemble} shows the call syntax of \lstinline|ensemble.py|.

\bash[stdoutFile=ensemble.tex,ignoreStderr=true]
../src/ensemble.py -h
\END

\lstinputlisting[label=lst:ensemble,style=BashOutputStyle,caption=ensemble.py usage]{ensemble.tex}

% --------------------------------------------------------------------------------------------------

\newcommand{\mixdefault}{\texttt{mixdefault}\xspace}
\newcommand{\nb}{\texttt{nb}\xspace}
\newcommand{\knn}{\texttt{knn}\xspace}
\newcommand{\svm}{\texttt{svm}\xspace}
\newcommand{\tree}{\texttt{tree}\xspace}
\newcommand{\extraforest}{\texttt{extraforest}\xspace}

\subsection{Methodology}

The classifiers for our main experiments were configured in the
\verb|conf/default.conf| configuration file. Six different ensemble classifiers
were tested, each in both simple and weighted majority vote variations:

\begin{itemize}
\item \mixdefault consists of one of each base classifier in its default configuration.
\item \nb combines seven naive bayes classifiers, each trained on a random sample
      of the training set.
\item \knn combines seven nearest neighbor classifiers with considered neighbors ranging
      from one to nine.
\item \svm consists of SVM classifiers with varying error term penalties.
\item \tree combines several tree variations with differing node splitters and criteria.
\item \extraforest combines ten extremely random trees trained on a random sample of the training set.
\end{itemize}

Each of these (and all of their component classifiers) were run on all
datasets using 5-fold cross-valdition. We report on training- and evaluation times,
together with mean and standard deviation of accuracies.

% --------------------------------------------------------------------------------------------------

\section{Results}

%TODO
\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_accuracy(c("../results/digits.csv", "../results/hastie.csv"),
		c("mix_default_simple_ensemble", "nb7_simple_ensemble", "knn7_simple_ensemble",
			"svm7_simple_ensemble", "tree7_simple_ensemble", "extraforest_simple_ensemble",
			"mix_default_weighted_ensemble", "nb7_weighted_ensemble", "knn7_weighted_ensemble",
			"svm7_weighted_ensemble", "tree7_weighted_ensemble", "extraforest_weighted_ensemble"))
@
\caption{test \label{fig:results_test}}
\end{figure}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_train_time(c("../results/digits.csv", "../results/hastie.csv"),
		c("mix_default_simple_ensemble", "nb7_simple_ensemble", "knn7_simple_ensemble",
			"svm7_simple_ensemble", "tree7_simple_ensemble", "extraforest_simple_ensemble",
			"mix_default_weighted_ensemble", "nb7_weighted_ensemble", "knn7_weighted_ensemble",
			"svm7_weighted_ensemble", "tree7_weighted_ensemble", "extraforest_weighted_ensemble"))
@
\caption{test2 \label{fig:results_test2}}
\end{figure}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_test_time(c("../results/digits.csv", "../results/hastie.csv"),
		c("mix_default_simple_ensemble", "nb7_simple_ensemble", "knn7_simple_ensemble",
			"svm7_simple_ensemble", "tree7_simple_ensemble", "extraforest_simple_ensemble",
			"mix_default_weighted_ensemble", "nb7_weighted_ensemble", "knn7_weighted_ensemble",
			"svm7_weighted_ensemble", "tree7_weighted_ensemble", "extraforest_weighted_ensemble"))
@
\caption{test3 \label{fig:results_test3}}
\end{figure}

% --------------------------------------------------------------------------------------------------

\section{Conclusion}

%TODO

\end{document}
