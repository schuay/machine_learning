\documentclass[usenames,dvipsnames]{beamer}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{comment}
%\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{xspace}

\lstset{
    language=Python,
    basicstyle=\ttfamily,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\title{Exercise 1 \\
       Machine Learning WS 2014/2015 \\
       Technical University of Vienna}
\author{Jakob Gruber, 0203440 \\
        Mino Sharkhawy, 1025887}

\newcommand{\annealing}{\texttt{annealing}\xspace}
\newcommand{\housing}{\texttt{housing}\xspace}
\newcommand{\powerconsumption}{\texttt{power}\xspace}
\newcommand{\twitter}{\texttt{twitter}\xspace}

\newcommand{\tenfold}{\texttt{10fold}\xspace}
\newcommand{\ratiostd}{\texttt{ratio75}\xspace}

\newcommand{\aesid}{\texttt{aes\_id}\xspace}
\newcommand{\noscale}{\texttt{noscale}\xspace}

\begin{document}

\maketitle


\section{Classifiers} \label{sec:classifiers}

\begin{frame}{\nameref{sec:classifiers}}

\begin{itemize}
\item \texttt{scikit-learn, python2-nltk, R}, \LaTeX
\item Naive Bayes: Popular baseline classifier, efficient, effective.
\item Support Vector Machine: effective for many attributes, small classifier size,
      expensive to train and evaluate.
\item K-nearest Neighbors: Remembers all training instances, simple but effective.
\end{itemize}
\end{frame}

\section{\annealing} \label{sec:annealing}
\begin{frame}{\nameref{sec:annealing}}

\begin{itemize}
\item \annealing ties properties of the steal annealing process to the resulting
      steel class.
\item For example: \texttt{Family: TN, Product type: C, Shape: COIL, Thickness: 1.6}
      results in a steel class of \texttt{5}.
\item Around 800 instances, 40 categorical, integer, and real features with many
      missing values, 6 classes.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:annealing}}
\framesubtitle{Main results}

\begin{itemize}
\item \emph{Very} effective: 100\% accuracy reached with small training sets.
\item Preprocessing was unnecessary: classifiers performed as well with all
      features as strings (even reals and integers).
\item If features converted to numerics, then some classifiers (SVM) require normalization.
\item In general, Bayes is very efficient to train and evaluate, but not as
      accurate as SVM.
\item SVM is expensive, but very accurate.
\item KNN is decent in all areas, but classifier size grows linearly with training set.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:annealing}}
\framesubtitle{Accuracy}

\begin{figure}
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_accuracy("../results/annealing_bayes_ratiorange.csv",
                          "../results/annealing_svm_ratiorange.csv",
                          "../results/annealing_knn_ratiorange.csv")
@
\caption{Classifier accuracy for varying training-evaluation ratios on the \annealing dataset.
         \label{fig:results_annealing_split_accuracy}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:annealing}}
\framesubtitle{Classifier Size}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/annealing_bayes_ratiorange.csv",
                      "../results/annealing_svm_ratiorange.csv",
                      "../results/annealing_knn_ratiorange.csv")
@
\caption{Classifier sizes for varying training-evaluation ratios on the \annealing dataset.
         \label{fig:results_annealing_split_size}}
\end{figure}
\end{frame}

% ------------------------------------------------------------------------------

\section{\twitter} \label{sec:twitter}
\begin{frame}{\nameref{sec:twitter}}

\begin{itemize}
\item \twitter are categorized into positive and negative sentiments.
\item For example: \texttt{"HUGE roll of thunder just now...SO scary!!!!"} is \texttt{Negative}.
\item N-gram features: \texttt{(HUGE, roll), (roll, of), \ldots}
\item Preprocessing: Spam filtering, multicharacter normalization
      (\texttt{HUUUUUGE} $\rightarrow$ \texttt{HUUGE}), user tokenization (\texttt{@JohnWayne} $\rightarrow$ \texttt{USER\_TOKEN}), \ldots
\item 1.5 million instances, very large feature set, no missing values, binary classification.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:twitter}}
\framesubtitle{Main results}

\begin{itemize}
\item Results comparable to best of literature: more than 80\% accuracy reached.
\item Two variations: \aesid (Stopword filter) and standard (No stopword filter,
      multicharacter normalization, user tokenization).
\item Feature set explosion, very demanding on classifiers (KNN unusable).
\item Preprocessing helps some classifiers, hurts others.
\item Bayes much larger than SVM classifier.
\item Stopword filter (filters common sentiment-less words) has small impact
      on accuracy, huge impact on SVM training time.
\item SVM wins again on classifier size and accuracy.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:twitter}}
\framesubtitle{Accuracy and runtime}

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Classifier & Train. T. (s) & Eval. T. (s) & Acc. \\
\midrule
Bayes (\ratiostd) & 27.36 & 29.268 & 0.78 \\
Bayes (\ratiostd, \aesid) & 29.63 & 20.69 & 0.78 \\
SVM (\ratiostd)   & 272.87 & 49.11 & 0.8  \\
SVM (\ratiostd, \aesid)   & 164.66 & 45.83 & 0.79  \\
Bayes (\tenfold)  & 31.87 & 12.0 & 0.77 \\
Bayes (\tenfold, \aesid)  & \textbf{33.9} & 8.24 & 0.77 \\
SVM (\tenfold)    & \textbf{322.89} & 20.02  & 0.79  \\
SVM (\tenfold, \aesid)    & \textbf{185.28} & 18.29  & 0.78  \\
\bottomrule
\end{tabular}
\caption{General results for the \twitter dataset. Cross-validation results are averaged over all
         partitions. \label{fig:results_twitter_summary}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:twitter}}
\framesubtitle{Accuracy}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_accuracy("../results/twitter_bayes_ratiorange.csv",
                          "../results/twitter_svm_ratiorange.csv",
                          NULL)
@
\caption{Classifier accuracy for varying training-evaluation ratios on the \twitter dataset.
         \label{fig:results_twitter_split_accuracy}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:twitter}}
\framesubtitle{Classifier Size}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/twitter_bayes_ratiorange.csv",
                      "../results/twitter_svm_ratiorange.csv",
                      NULL)
@
\caption{Classifier sizes for varying training-evaluation ratios on the \twitter dataset.
         \label{fig:results_twitter_split_size}}
\end{figure}
\end{frame}

% ------------------------------------------------------------------------------

\section{Regressors} \label{sec:regressors}
\begin{frame}{\nameref{sec:regressors}}

\begin{itemize}
\item \texttt{scikit-learn, R}, \LaTeX
\item Stochastic Gradient Descent (SGD): Very fast, works on huge datasets.
\item Support Vector Machine (SVR): as with SVM classifier, very expensive
\end{itemize}
\end{frame}

\section{\powerconsumption} \label{sec:powerconsumption}
\begin{frame}{\nameref{sec:powerconsumption}}

\begin{itemize}
\item \powerconsumption contains minute-by-minute records of power consumption in one
      household.
\item For example: \texttt{Date: 16/12/2006, Time: 17:24:00, Global\_active\_power: 4.216,
      Global\_passive\_power: 0.418, Voltage: 234.840, Global\_intensity: 18.400,
      Sub\_metering\_1: 0.000, Sub\_metering\_2: 1.000, Sub\_metering\_3: 17.000}.
\item Over 2 million instances, date and time, real features, some missing values.
\item Predict Global\_active\_power using date and time.
\item Missing values replaced with mean.
\item Features scaled to fit Gaussian standard distribution.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:powerconsumption}}
\framesubtitle{Main results}

\begin{itemize}
\item Poor results, $R^2$ score only around 25\%
\item Preprocessing was necessary to deal with date and time. Date was split into 7 binary
      features, one for each weekday. Time was split into hours and minutes.
\item SGD performs better than SVR when scaling is used. Without scaling SVR is much
      better.
\item SVR model is extremely slow with large training dataset. We could only test with
      100.000 instances.
\item Different preprocessing methods yielded no better results.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:powerconsumption}}
\framesubtitle{Mean squared error, $R^2$ score and runtime}

\begin{figure}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
Regressor & Train. T. (s) & Eval. T. (s) & MSE & $R^2$ \\
\midrule
SGD (\ratiostd) & 0.27 & 0.03 & 1.54 & 0.14 \\
SVR (\ratiostd) & 376.35 & 53.37 & 1.55 & 0.14 \\
SVR (\noscale, \ratiostd) & 561.47 & 54.24 & 1.36 & 0.24 \\
SGD (\tenfold) & \textbf{0.36} & 0.01 & 1.54 & 0.15 \\
SVR (\tenfold) & \textbf{516.37} & 25.71 & 1.55 & \textbf{0.14} \\
SVR (\noscale, \tenfold) & \textbf{807.86} & 24.93 & 1.35 & \textbf{0.25} \\
\bottomrule
\end{tabular}
\caption{General results for the \powerconsumption dataset, with binary weekday features.
         All attributes are scaled to fit a Gaussian standard distribution, unless they
	 use \noscale. Cross-validation results are averaged over all partitions.
	 \label{fig:results_powerconsumption_summary}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:powerconsumption}}
\framesubtitle{$R^2$ score}

\begin{figure}
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_r2("../results/power_consumption_sgd_ratiorange.csv",
                    "../results/power_consumption_svr_ratiorange.csv",
                    "../results/power_consumption_svr_ratiorange_noscale.csv")
@
\caption{$R^2$ score for varying training-evaluation ratios on the \powerconsumption dataset.
         \label{fig:results_powerconsumption_split_r2}}
\end{figure}
\end{frame}

% ------------------------------------------------------------------------------

\section{\housing} \label{sec:housing}
\begin{frame}{\nameref{sec:housing}}

\begin{itemize}
\item \housing contains information of different suburban areas.
\item For example: \texttt{CRIM: 0.00632, ZN: 18.00, INDUS: 2.310, CHAS: 0, NOX: 0.5380,
      RM: 6.5750, AGE: 65.20, DIS: 4.0900, RAD: 1, TAX: 296.0, PTRATIO: 15.30, B: 396.90,
      LSTAT: 4.98, MEDV: 24.00}.
\item Attempt to predict median housing value (MEDV) in such an area.
\item Only 506 instances, 14 features, categorical, integer and real.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:housing}}
\framesubtitle{Main results}

\begin{itemize}
\item Almost no preprocessing required as all features can be represented by numbers.
\item Scaling to Gaussian standard distribution vastly improves performance. Results here
      use scaling.
\item Almost no difference between SGD and SVR for large training sets.
\item SGD very bad with small training sets.
\end{itemize}
\end{frame}

\begin{frame}{\nameref{sec:housing}}
\framesubtitle{Mean squared error, $R^2$ score and runtime}

\begin{figure}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
Regressor & Train. T. (s) & Eval. T. (s) & MSE & $R^2$ \\
\midrule
SGD (\ratiostd) & 0.00243 & 0.00036 & 26.60676 & 0.69566 \\
SVR (\ratiostd) & 0.0142 & 0.00237 & 27.67327 & 0.68346 \\
SGD (\tenfold) & 0.002535 & 0.000234 & 24.6665 & 0.680654 \\
SVR (\tenfold) & 0.019568 & 0.001218 & 28.3543 & 0.65123 \\
\bottomrule
\end{tabular}
\caption{General results for the \housing dataset. Cross-validation results are averaged over all
         partitions. \label{fig:results_housing_summary}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:housing}}
\framesubtitle{$R^2$ score}

\begin{figure}
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_r2("../results/housing_sgd_ratiorange.csv",
                    "../results/housing_svr_ratiorange.csv",
		    NULL)
@
\caption{$R^2$ score for varying training-evaluation ratios on the \housing dataset.
         \label{fig:results_housing_split_r2}}
\end{figure}
\end{frame}

% ------------------------------------------------------------------------------

\section{Conclusion} \label{sec:conclusion}

\begin{frame}{Q \& A}
\centering
Questions?
\end{frame}

% ------------------------------------------------------------------------------

\section{Additional Material} \label{sec:addl_material}

\begin{frame}{\nameref{sec:annealing}}
\framesubtitle{Accuracy and runtime}

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Classifier & Train. Time (s) & Eval. Time (s) & Accuracy \\
\midrule
Bayes (\ratiostd) & 0.0077 & 0.026 & 0.95 \\
SVM (\ratiostd)   & 0.0224 & 0.024 & 1.0  \\
KNN (\ratiostd)   & 0.016  & 0.143 & 0.97 \\
Bayes (\tenfold)  & 0.0091 & 0.011 & 0.96 \\
SVM (\tenfold)    & 0.0263 & 0.01  & 1.0  \\
KNN (\tenfold)    & 0.0182 & 0.058 & 0.97 \\
\bottomrule
\end{tabular}
\caption{General results for the \annealing dataset without preprocessing. All instance attributes are
         treated as strings. Cross-validation results are averaged over all
         partitions. \label{fig:results_annealing_summary}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:powerconsumption}}
\framesubtitle{Mean squared error}

\begin{figure}
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_mse("../results/power_consumption_sgd_ratiorange.csv",
                     "../results/power_consumption_svr_ratiorange.csv",
                     "../results/power_consumption_svr_ratiorange_noscale.csv")
@
\caption{Mean squared error for varying training-evaluation ratios on the \powerconsumption dataset.
         \label{fig:results_powerconsumption_split_mse}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:powerconsumption}}
\framesubtitle{Regressor Size}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/power_consumption_svr_ratiorange.csv",
                      NULL, NULL, "regressor")
@
\caption{Regressor sizes for varying training-evaluation ratios on the \powerconsumption dataset.
         The SGD regressor could not be measured.
         \label{fig:results_powerconsumption_split_size}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:housing}}
\framesubtitle{Mean squared error}

\begin{figure}
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_mse("../results/housing_sgd_ratiorange.csv",
                     "../results/housing_svr_ratiorange.csv",
		    NULL)
@
\caption{Mean squared error for varying training-evaluation ratios on the \housing dataset.
         \label{fig:results_housing_split_mse}}
\end{figure}
\end{frame}

\begin{frame}{\nameref{sec:housing}}
\framesubtitle{Regressor Size}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/housing_svr_ratiorange.csv",
                      NULL, NULL, "regressor")
@
\caption{Regressor sizes for varying training-evaluation ratios on the \housing dataset.
         The SGD regressor could not be measured.
         \label{fig:results_housing_split_size}}
\end{figure}
\end{frame}

\end{document}
