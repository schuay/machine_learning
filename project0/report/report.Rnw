\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{bashful}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{xspace}

\lstset{
    language=Python,
    basicstyle=\ttfamily,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\lstdefinestyle{BashInputStyle}{
  language=bash,
  firstline=2,% Supress the first line that begins with `%`
  basicstyle=\small\sffamily,
  numbers=left,
  numberstyle=\tiny,
  numbersep=3pt,
  frame=tb,
  columns=fullflexible,
  backgroundcolor=\color{yellow!20},
  linewidth=0.9\linewidth,
  xleftmargin=0.1\linewidth
}

\lstdefinestyle{BashOutputStyle}{
  basicstyle=\small\ttfamily,
  numbers=none,
  frame=tblr,
  columns=fullflexible,
  backgroundcolor=\color{gray!10},
}

\title{Exercise 1 \\
       Machine Learning WS 2014/2015 \\
       Technical University of Vienna}
\author{Jakob Gruber, 0203440 \\
        Mino Sharkhawy, 1025887}

\newcommand{\annealing}{\texttt{annealing}\xspace}
\newcommand{\powerconsumption}{\texttt{power\_consumption}\xspace}
\newcommand{\housing}{\texttt{housing}\xspace}
\newcommand{\twitter}{\texttt{twitter}\xspace}

\newcommand{\tenfold}{\texttt{10fold}\xspace}
\newcommand{\ratiostd}{\texttt{ratio75}\xspace}
\newcommand{\ratiorange}{\texttt{ratiorange}\xspace}

\newcommand{\aesid}{\texttt{aes\_id}\xspace}
\newcommand{\noscale}{\texttt{noscale}\xspace}

\begin{document}

\maketitle

\section{Introduction}

This assignment consists of:

\begin{itemize}
\item picking a good, diverse selection of datasets,
\item choosing several suitable classifier and regression techniques,
\item analyzing the behavior of the latter when run on the former,
\item while experimenting with different preprocessing techniques,
\item and finally reporting on the results (you are feasting your eyes on this
      artifact right now).
\end{itemize}

% --------------------------------------------------------------------------------------------------

\section{Dataset Selection}

When selecting our datasets, we selected mainly for diversity in dataset features:
if one dataset had few instances, we preferred sets with a huge number of instances
as the second selection; if one set had only categorical attributes, we tried to
find a second set with integer and/or real attributes, etc. Finally, we also
preferred datasets that simply sounded interesting.

Our classification datasets are \annealing, a small dataset with a medium
number of multivariate attributes and partially missing values which ties various
attributes of the steel annealing process to the produced class of steel;
and \twitter, a huge dataset of tweets in which we interpreted words (or combinations of words)
as binary features. The purpose of the \twitter dataset is to predict the sentiment
of a tweet, i.e. if the tweet carries a positive or negative emotion.
Full disclosure: we have previously experimented on this dataset
in the \emph{Advanced Internet Computing} lecture.

Our regression datasets are \housing, with a small number of instances and a medium number
of categorical, integer and real features, which predicts the median value of
owner-occupied homes. Finally, the \powerconsumption dataset contains
a huge number of instances, and a low number of attributes (with missing values).
Some of the attributes are time-based, leading to interesting choices in preprocessing,
while the others are real values.

For further dataset details, see Figure \ref{fig:datasets}.

\begin{comment}
solar flares:
* interesting since regression on categorical data

powerconsumption:
* coupled attributes -> reduce only to time. choice of preprocessing important.
\end{comment}


\begin{figure}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\multirow{2}[4]{*}{Datasets} & \multicolumn{2}{c}{Classification} & \multicolumn{2}{c}{Regression} \\
\cmidrule(l){2-3} \cmidrule(l){4-5}
                & \twitter     & \annealing     & \housing       & \powerconsumption \\
\midrule
Nr. of samples  & Huge         & Medium         & Small          & Huge              \\
Nr. of features & Huge         & Medium         & Medium         & Low               \\
Feature types   & Binary       & Cat, Int, Real & Cat, Int, Real & Date, Real        \\
Missing values  & No           & Yes            & No             & Yes               \\
Preprocessing   & Beneficial   & Optional       & None           & Reqired           \\
Result set      & \{Pos, Neg\} & 6 classes      & Real           & Real              \\
\bottomrule
\end{tabular}
\caption{Dataset details. Feature types are abbreviated as: Bin = binary, Cat = categorical,
         Int = integral. \label{fig:datasets}}
\end{figure}

\section{Classifier Selection}

We selected our classification techniques for familiarity (i.e. we preferred techniques covered
in the lecture) and suitability for our classification datasets.

The first classifier is a Support Vector Machine\footnote{\lstinline|sklearn.svm.LinearSVC|}, which attempts
to learn a function maximizing its margin from so-called support vectors. SVM's are effective
for datasets with high dimensions and a large number of attributes, and reduce classifier sizes by
relying only on a subset of data points (the support vectors). However, they can be expensive
both to train and apply.

Naive bayes\footnote{\lstinline|nltk.classify.NaiveBayesClassifier|} was selected as the second classifier
due both to its simplicity and the fact that it seems to be a popular standard baseline classifier.
It assumes feature independence, and is known to perform well in document classification. Both training
and evaluation are time-efficient.

Finally, we chose k-nearest neighbors\footnote{\lstinline|sklearn.neighbors.KNeighborsClassifier|} as our
third classification technique. Nearest neighbors is a non-generalizing classifier, i.e. it does not
attempt to learn a general function, but instead simply memorizes all seen training data points.
Prediction consists simply of examining the $k$ nearest neighbors. This classifier again performs
surprisingly well despite its simplicity, but obviously grows linearly in the training set size.

\section{Regressor Selection}

The first regressor we chose\footnote{\lstinline|sklearn.linear_model.SGDRegressor|} uses Stochastic Gradient Descent.
This technique attempts to minimize a loss function that is written as a sum of differentiable functions.
This regressor is particularly useful for large datasets and is faster than many of the alternatives. Additionally,
we were already familiar with Gradient Descent from the lecture.

Our second regressor is again a Support Vector Machine\footnote{\lstinline|sklearn.svm.SVR|}.
Everything said for the SVM classifier also applies here. This regressor is very time comsuming when compared to the
SGD regressor above and using it on large datasets is very time consuming.

% --------------------------------------------------------------------------------------------------

\section{Tools}

We based our experiments on the machine learning toolkit
\verb|scikit-learn 0.15.2|\footnote{\url{http://scikit-learn.org/stable/}}
in conjunction with the natural language toolkit
\verb|nltk 3.0a4|\footnote{\url{http://www.nltk.org/}}.

The entire application logic
(loading, preprocessing, training, and evaluation) is written in Python, giving us
maximal flexibility.

Results were exported to CSV files, which were then used to generate graphs
using R and the \verb|ggplot2|\footnote{\url{http://ggplot2.org/}} package.

Obviously, this report was typeset in \LaTeX (and less obviously, integrated with
R using \verb|sweave|\footnote{\url{https://www.stat.uni-muenchen.de/~leisch/Sweave/}}).

% --------------------------------------------------------------------------------------------------

\section{Results}

Benchmarks for the classification datasets \annealing and \twitter were performed on a a machine
with 8 GB of RAM and an Intel Core i7-3770 CPU at 3.40 GHz, running Linux 3.17.4.
The \powerconsumption and \housing benchmarks were performed on a machine with 64 GB of RAM and
2 Intel Xeon E5-2687W processors, running Linux 3.14.3.

\subsection{Toolchain}

The \lstinline|data| directory contains subdirectories for the used datasets. By executing \lstinline|make| in
\lstinline|data| all datasets are downloaded to their respective directories. Each dataset is represented by a
class in the \lstinline|src| directory.

To evaluate our classifiers and regressors, the \lstinline|classifier.py| and \lstinline|regressor.py| scripts
in \lstinline|src| are used. Both scripts allow the user to specify a dataset, a model, an evaluation strategy
(here called splitter) and a limit on the number of samples to use from the dataset.

Listing~\ref{lst:classifier} and Listing~\ref{lst:regressor} show the call syntax of \lstinline|classifier.py|
and \lstinline|regressor.py| respectively.

\bash[stdoutFile=classifier.tex]
../src/classifier.py -h
\END

\lstinputlisting[label=lst:classifier,style=BashOutputStyle,caption=classifier.py usage]{classifier.tex}

\bash[stdoutFile=regressor.tex]
../src/regressor.py -h
\END

\lstinputlisting[label=lst:regressor,style=BashOutputStyle,caption=regressor.py usage]{regressor.tex}

Once finished, \lstinline|classifier.py| and \lstinline|regressor.py| will print their
results in a CSV format. The output contains the dataset, the name of the classifier or
regressor, the splitter, the size of the training and evaluation datasets, the time needed
for training and evaluation and the size of the model as well as several performance
metrics.

\subsection{Evaluation strategies}

We used three different evaluation strategies which we called splitters due to the fact
that they are used to split a given dataset into a training and an evaluation set.

\begin{description}
    \item[\ratiorange] This method was primarilary used to generate the graphs. First the
    dataset is split such that 5\% are used as training data and the rest is used for
    evaluation. Then the split is increased by 5\% and the experiment is repeated. This
    continues until 95\% of the dataset are used for training purposes.
    \item[\ratiostd] In this evaluation method, a single experiment is run. 75\% of the
    dataset serve as training data, the remaining 25\% are used for evaluation.
    \item[\tenfold] This strategy performs 10-fold cross-validation.
\end{description}

In addition to splitting the dataset into a training and an evaluation set, a splitter
also shuffles the data to prevent problems like the one described in the beginning of
Section~\ref{sec:powerconsumption}

\subsection{\annealing}

Evaluation of the \annealing dataset yielded very good results in all circumstances,
even without preprocessing.

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Classifier & Train. Time (s) & Eval. Time (s) & Accuracy \\
\midrule
Bayes (\ratiostd) & 0.0077 & 0.026 & 0.95 \\
SVM (\ratiostd)   & 0.0224 & 0.024 & 1.0  \\
KNN (\ratiostd)   & 0.016  & 0.143 & 0.97 \\
Bayes (\tenfold)  & 0.0091 & 0.011 & 0.96 \\
SVM (\tenfold)    & 0.0263 & 0.01  & 1.0  \\
KNN (\tenfold)    & 0.0182 & 0.058 & 0.97 \\
\bottomrule
\end{tabular}
\caption{General results for the \annealing dataset without preprocessing. All instance attributes are
         treated as strings. Cross-validation results are averaged over all
         partitions. \label{fig:results_annealing_summary}}
\end{figure}

Our main results were obtained entirely
without preprocessing: all instance attributes (even real-valued ones such as length or thickness)
were treated as strings, and missing values were simply omitted. Conversion of real-valued attributes
to \lstinline|float| yielded no change in the Bayes classifier (since Bayes does not
distinguish between continuous and categorical data), and resulted in a degradation of accuracy of
between 10\% (KNN) and 20\% (SVM). Subsequent normalization of large real-valued attributes such as
length and width negated this effect, and caused the accuracy of both KNN and SVM classifiers to
return to their previous state as shown in Figure \ref{fig:results_annealing_summary}. Replacing missing
values by a dedicated value resulted in a degradation of 3\% for the Bayes classifier, and negligible
changes for SVM and KNN. We thus decided
to perform the remaining experiments without preprocessing, since results seemed optimal and the pipeline
was simpler.

Figure \ref{fig:results_annealing_summary} gives an overview of classifier performance and accuracy
for both a 75-25\% split between training and evaluation data, and using 10-fold cross-validation.
Accuracy is surprisingly high for in all cases, with all classifiers predicting at least 95\% of all
evaluation instances correctly. The SVM classifier in particular predicts optimally without making a
single mistake.

Furthermore, it stands out, that:

\begin{itemize}
\item the Bayes classifier is time-efficient both in training and classification,
\item and that the SVM is very time-intensive.
\end{itemize}

\begin{figure}[ht]
\centering
\begin{tabular}{ccc}
\toprule
Bayes & SVM & KNN \\
\midrule
24576 & 69632 & 1011712 \\
\bottomrule
\end{tabular}
\caption{Trained classifier sizes in bytes for the \annealing dataset and a training-evaluation ratio of 75\%.
         \label{fig:results_annealing_sizes}}
\end{figure}

We attempted to determine the space required by the different classifiers by using the python serialization
module \lstinline|cPickle|, pickling the trained classifiers, and finally determining the produced file's
size. The results are displayed in Figure \ref{fig:results_annealing_sizes}. In this case, it is especially
notable that the KNN classifier is larger than the others by over a factor of $14$. It is also somewhat
surprising that SVM is larger than Bayes, since we clearly obtained the opposite result on the \twitter
dataset, in which SVM offered a significant size advantage. Figure \ref{fig:results_annealing_split_size}
reveals that the size of the KNN classifier grows linearly with the number of training instances, while
SVM and Bayes classifiers stayed more or less at a constant size.

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Class & Class Size & Precision & Recall \\
\midrule
1 & 0 & N/A & N/A \\
2 & 23 & 1.0 & 0.78 \\
3 & 155 & 0.97 & 0.96 \\
4 & 0 & N/A & N/A \\
5 & 17 & 1.0 & 1.0 \\
U & 5 & 0.45 & 1.0 \\
\bottomrule
\end{tabular}
\caption{Precision and recall results for the \annealing dataset, using the Bayes classifier and
         \ratiostd partitioning. Class sizes are given as the number of instances of a particular class
         within the evaluation set. \label{fig:results_annealing_per_class}}
\end{figure}

Figure \ref{fig:results_annealing_per_class} shows precision and recall statistics for a particular
execution of the Bayes classifier. Class 3 is the most common by far, and is predicted very accurately.
Classes 1 and 4 do not appear at all in this selection of evaluation instances, and the smallest class
U exhibits poor precision, i.e. over half of the instances classified as U are predicted incorrectly.

Finally, since all classifiers were overwhelmingly positive, we experimented with altering the ratio
of training to evaluation instances. Figures \ref{fig:results_annealing_split_accuracy} and
\ref{fig:results_annealing_split_size} show, respectively, the produced accuracy
and classifier sizes of the SVM, KNN, and Bayes classifiers given a certain training-evaluation ratio
ranging from 5\% to 95\%. SVM is the clear winner in this dataset, rising to 98\% accuracy for
training-evaluation ratios as low as 15\% (i.e. training on around 120 instances). SVM reaches
100\% accuracy at a ratio of 37\%, while both Bayes and KNN keep improving slowly with rising
ratios.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_accuracy("../results/annealing_bayes_ratiorange.csv",
                          "../results/annealing_svm_ratiorange.csv",
                          "../results/annealing_knn_ratiorange.csv")
@
\caption{Classifier accuracy for varying training-evaluation ratios on the \annealing dataset.
         \label{fig:results_annealing_split_accuracy}}
\end{figure}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/annealing_bayes_ratiorange.csv",
                      "../results/annealing_svm_ratiorange.csv",
                      "../results/annealing_knn_ratiorange.csv")
@
\caption{Classifier sizes for varying training-evaluation ratios on the \annealing dataset.
         \label{fig:results_annealing_split_size}}
\end{figure}

The \annealing dataset is obviously very well suited to classification using machine learning and produces
exceptional accuracy even with a low number of training instances. The data is skewed heavily towards
a single class, with 75\% of all instances belonging to class 3 (and, for example, none to class 4).
We assume that this is a large part of the reason why our classifiers perform so well; in fact, a naive classifier
that simply predicts class 3 every time would also have 75\% accuracy. It was especially instructive
to see the different behaviors of the classifiers both in training and evaluation times, and in
the size of the produced files. Normalization of real-valued attributes also had a larger than expected
effect on the classifier accuracy.

% --------------------------------------------------------------------------------------------------

\subsection{\twitter}

The \twitter dataset is very large in several respects; it not only contains over 1.5 million
tweets, but by it's very nature has a huge number of feature (at least one per mentioned word).
Employed alternative feature selection methods such as n-grams, which map each occurring combination
words of length $\leq n$ to a feature, quickly increases the number of features into infeasable areas.
Memory and CPU time constraints definitely limit the kinds of experiments which can be conducted
with this type of dataset.

We employed various methods of preprocessing and feature selection on this dataset:

\begin{itemize}
\item Spam filtering: \twitter contains a large number of nearly identical spam tweets, which we
      simply remove at the beginning of the pipeline.
\item Stopword filtering: Many words of the english language such as 'me', 'you', 'it', 'or',
      etc., occur frequently but do not contribute any sentiment. Such so called stopwords are removed
      to decrease the size of the feature set. It is however important that common words which \emph{do}
      carry sentiment ('no', 'very', 'but').
\item Emoticon selection: Selection of all emoticons as features.
\item N-gram selection: Selection of all combination of words of length $\leq n$ as features. N-grams
      enable handling of phrases such as 'not good', which obviously carries more accurate meaning
      than both words separately.
\item URL and user transformers: URLs and usernames are presumed not to carry sentiment. This transformation
      replaces all such occurrences with a single feature.
\item Multicharacter normalization: In informal settings such as Twitter, words are often emphasized
      by repeating letters (i.e. "I'm feeling goooood"). Multicharacter normalization replaces all
      sequences of more than two repeated characters with exactly two of that character in order to
      shrink the feature set.
\end{itemize}

Feature selection results in a specific feature set, all of which may take the integer values 0 or 1
(one can alternatively see them as booleans).

Through experimental evaluation, we determined that a combination of multicharacter normalization,
user transformer, and n-gram as well as emoticon selection produced the best accuracy on the \twitter
dataset. Both the stopword filter and the URL transformer resulted in a slight decrease of accuracy.
While larger n-grams increased accuracy, they caused the size of the feature set to grow explosively
and ultimately led to us running out of memory with $n = 3$. Unless stated otherwise, the following
benchmarks are run using the aforementioned configuration of feature selectors and transformers; even
though larger n-grams improved accuracy, we used 1-grams in order to avoid running out of memory and
vastly increased runtimes. The KNN classifier had to be left out of larger experiments since it
would not finish executing even after over 12 hours.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_accuracy("../results/twitter_bayes_ratiorange.csv",
                          "../results/twitter_svm_ratiorange.csv",
                          NULL)
@
\caption{Classifier accuracy for varying training-evaluation ratios on the \twitter dataset.
         \label{fig:results_twitter_split_accuracy}}
\end{figure}

Figures \ref{fig:results_twitter_split_accuracy} and \ref{fig:results_twitter_split_size} display
classifier accuracy and serialization sizes for varying training-evaluation ratios. Results for
benchmarks using the stopwords filter and no transformers are included for the sake of comparison
and are labelled with a \aesid suffix.

The accuracy of
both SVM and Bayes behaves similarly, increasing steadily up to a ratio of 90\%. Accuracy decreases
again at 95\%; we were not able to determine the cause conclusively, but speculate that this was caused
by properties of the dataset. We reached an accuracy of 80.9\% with the SVM classifier, which, according to the literature\footnote{A. Go, R. Bhayani, and L. Huang.
Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1-12, 2009.
}\footnote{J. Lin and A. Kolcz. Large-scale machine learning at twitter. In Proceedings of the 2012
ACM SIGMOD International Conference on Management of Data, SGIMOD '12, pages 793-804, New York, NY,
USA, 2012. ACM.}, compares favorably to top results achieved by others within the same area.
This demonstrates that good results may be achieved by very simple means, in this case the SVM classifier
provided by \texttt{scikit-learn} and basic preprocessing. It is especially
interesting that while all classifiers and configurations produce similar results,
\aesid is straddled by the standard configuration instead of having both
Bayes and SVM produce either better or worse accuracies.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/twitter_bayes_ratiorange.csv",
                      "../results/twitter_svm_ratiorange.csv",
                      NULL)
@
\caption{Classifier sizes for varying training-evaluation ratios on the \twitter dataset.
         \label{fig:results_twitter_split_size}}
\end{figure}

Classifier sizes differ significantly between Bayes and SVM, with the serialized Bayes classifier
being larger than the SVM classifier by a factor of around 4. This result is very much expected,
since the SVM classifier keeps only the support vectors and discards all other data points. Given the
classifier results from \annealing dataset in Figure \ref{fig:results_annealing_split_size}, we expect
the KNN classifier to have grown prohibitively large and thus see classifier size as the likely cause
for KNN's failure to complete. Compared to \aesid, the standard configuration produces
smaller classifiers (by around a factor of 2) due to feature reduction through user and multicharacter transformers.

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Classifier & Train. Time (s) & Eval. Time (s) & Accuracy \\
\midrule
Bayes (\ratiostd) & 27.36 & 29.268 & 0.78 \\
Bayes (\aesid, \ratiostd) & 29.63 & 20.69 & 0.78 \\
SVM (\ratiostd)   & 272.87 & 49.11 & 0.8  \\
SVM (\aesid, \ratiostd)   & 164.66 & 45.83 & 0.79  \\
Bayes (\tenfold)  & 31.87 & 12.0 & 0.77 \\
Bayes (\aesid, \tenfold)  & 33.9 & 8.24 & 0.77 \\
SVM (\tenfold)    & 322.89 & 20.02  & 0.79  \\
SVM (\aesid, \tenfold)    & 185.28 & 18.29  & 0.78  \\
\bottomrule
\end{tabular}
\caption{General results for the \twitter dataset. Cross-validation results are averaged over all
         partitions. \label{fig:results_twitter_summary}}
\end{figure}

Finally, training- as well as evaluation times and accuracy are again given in Figure
\ref{fig:results_twitter_summary} for both \ratiostd and \tenfold evaluation methods.
Accuracy is as expected after being thoroughly examined in Figure
\ref{fig:results_twitter_split_accuracy}. Bayes once again proves to be very
time-efficient both during training and evaluation (even though the produced
classifier is much larger than the SVM classifier). Training an SVM classifier
is expensive, taking between a factor of 5 and 10 longer than Bayes training.

Most surprising though was the effect that the stopword filter had on SVM training
times, adding over 100 seconds to the runtime in both \ratiostd and \tenfold
benchmarks. Even though both multichar and user transformers impacted the number
of features, they did not have this magnitude of effect by far. The difference
between the transformers and the stopword filter is that while transformers
significantly reduce the number of features with few occurrences each,
the stopword filter strips a small number of features with each having a very
large number of occurrences.

% --------------------------------------------------------------------------------------------------

\subsection{\powerconsumption}
\label{sec:powerconsumption}

The \powerconsumption dataset contains minute-by-minute records of the power consumption
in a single household over a period of almost four years. We try to estimate the gobal
active power for a given point in time. The dataset contains several other values, but
they are dependent on the global active power and thus we omitted them.

The time of a power measurement is given as a data string (e.g. 16/12/2006) and the clock
time (e.g. 17:24:00). Since the regressors expect numerical features, preprocessing is
necessary. Attempting to express the date and time as integers (i.e. the number of seconds
since the Unix epoch) yielded bad results. Therefore we settled with replacing the date
with seven binary features, each representing one day of the week. We use the hours and
minutes of the clock time as two separate integer features.

Additionally, we scale all feature values to fit a Gaussian standard distribution. This is
necessary for some Regressors to work properly. If we omit this standardization, this is
indicated by the \noscale keyword.

Due to the poor runtime performance of the SVR model, we were unable to test our
regressors with the entire dataset and therefore only used the first $100.000$ entries.

All regression algorithms performed pooly when the dataset was read in chonological order.
A possible explanation for this bahaviour is that by using a simple split between training
and evaluation data set, trends continue into the evaluation data set and can not be
anticipated by the model. To mitigate this problem, the dataset is shuffled before use.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_mse("../results/power_consumption_sgd_ratiorange.csv",
                     "../results/power_consumption_svr_ratiorange.csv",
                     "../results/power_consumption_svr_ratiorange_noscale.csv")
@
\caption{Mean squared error for varying training-evaluation ratios on the \powerconsumption dataset.
         \label{fig:results_powerconsumption_split_mse}}
\end{figure}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_r2("../results/power_consumption_sgd_ratiorange.csv",
                    "../results/power_consumption_svr_ratiorange.csv",
                    "../results/power_consumption_svr_ratiorange_noscale.csv")
@
\caption{$R^2$ score for varying training-evaluation ratios on the \powerconsumption dataset.
         \label{fig:results_powerconsumption_split_r2}}
\end{figure}

Figures \ref{fig:results_powerconsumption_split_mse} and
\ref{fig:results_powerconsumption_split_r2} show the mean squared error (MSE) and the
$R^2$ score of our two regressors for varying training-evaluation ratios. These two
metrics are commonly used to measure regression performance.

Both figures show that SGD performs consistently better than SVR when the aforementioned
scaling is used, however SVR without scaling performs much better than both.
Unfortunately, the SGD model did not produce any results with \noscale.

While the SGD regressor performs similarly regardless of the size of the training set, the
SVR shows a drastic decrease of the MSE and an increase or the $R^2$ score when the ratio
rises from 5\% to 10\%. The SVR's performance then improves slightly as the ratio rises.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/power_consumption_svr_ratiorange.csv",
                      NULL, NULL, "regressor")
@
\caption{Regressor sizes for varying training-evaluation ratios on the \powerconsumption dataset.
         \label{fig:results_powerconsumption_split_size}}
\end{figure}

Figure~\ref{fig:results_powerconsumption_split_size} shows the serialization size for the
SVR model for different training-evaluation ratios. It grows linearly with the size of the
training set. Curiously, the SGD regressor could not be properly serialized.

\begin{figure}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
Regressor & Train. Time (s) & Eval. Time (s) & MSE & $R^2$ \\
\midrule
SGD (\ratiostd) & 0.27284 & 0.03055 & 1.53938 & 0.14235 \\
SVR (\ratiostd) & 376.3459 & 53.3727 & 1.55024 & 0.1363 \\
SVR (\noscale, \ratiostd) & 561.47152 & 54.24232 & 1.35956 & 0.24254 \\
SGD (\tenfold) & 0.363914 & 0.0111 & 1.54207 & 0.145796 \\
SVR (\tenfold) & 516.374 & 25.7114 & 1.55244 & 0.140111 \\
SVR (\noscale, \tenfold) & 807.875 & 24.9298 & 1.35017 & 0.252097 \\
\bottomrule
\end{tabular}
\caption{General results for the \powerconsumption dataset. Cross-validation results are averaged over all
         partitions. \label{fig:results_powerconsumption_summary}}
\end{figure}

Table~\ref{fig:results_powerconsumption_summary} shows the training and evaluation times
as well as the MSE and $R^2$ score for our three models using the \ratiostd and \tenfold
evaluation methods. We can easily see that the SGD regressor is much faster than SVR.
Furthermore, the table shows consistently with the Figures
\ref{fig:results_powerconsumption_split_mse} and
\ref{fig:results_powerconsumption_split_r2} that the SVR without scaling performs much
better than SVR and SGD with scaling.

In general, the results we obtained are relatively bad. Even with SVR without scaling, the
$R^2$ score is only about 25\%. However, several attempts to improve these results, like
different representations of time, or performing the regression for each weekday
separately, failed to produce better results.

% --------------------------------------------------------------------------------------------------

\subsection{\housing}

The \housing dataset contains various features of housing areas. Given these features, we
aim to predict the median value of owner-occupied homes in that area.

We use all features this dataset provides. As all attributes already are real or integer
values and the only categorical attribute is binary, to preprocessing other than scaling
to a Gaussian standard distribution has been attempted. Omitting this scaling leads to
very bad results and therefore not \noscale regressor was used for this dataset.

As with the \powerconsumption dataset above, the results for this dataset are poor if the
dataset is not shuffled prior to being passed to the regressor.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_mse("../results/housing_sgd_ratiorange.csv",
                     "../results/housing_svr_ratiorange.csv",
		    NULL)
@
\caption{Mean squared error for varying training-evaluation ratios on the \housing dataset.
         \label{fig:results_housing_split_mse}}
\end{figure}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_r2("../results/housing_sgd_ratiorange.csv",
                    "../results/housing_svr_ratiorange.csv",
		    NULL)
@
\caption{$R^2$ score for varying training-evaluation ratios on the \housing dataset.
         \label{fig:results_housing_split_r2}}
\end{figure}

Figures \ref{fig:results_housing_split_mse} and \ref{fig:results_housing_split_r2} show
the MSE and $R^2$ score of the regressors for different training-evaluations ratios.

Both SGD and SVR perform similarly for large training sets. However, as with the
\powerconsumption dataset SVR performs very bad if the training set is too small.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/housing_svr_ratiorange.csv",
                      NULL, NULL, "regressor")
@
\caption{Regressor sizes for varying training-evaluation ratios on the \housing dataset.
         \label{fig:results_housing_split_size}}
\end{figure}

We can see in Figure~\ref{fig:results_housing_split_size} that the serialization size for
the SVR model grows, as before, linearly with the size of the training set. Again, the SGD
regressor could not be properly serialized.

\begin{figure}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
Regressor & Train. Time (s) & Eval. Time (s) & MSE & $R^2$ \\
\midrule
SGD (\ratiostd) & 0.00243 & 0.00036 & 26.60676 & 0.69566 \\
SVR (\ratiostd) & 0.0142 & 0.00237 & 27.67327 & 0.68346 \\
SGD (\tenfold) & 0.002535 & 0.000234 & 24.6665 & 0.680654 \\
SVR (\tenfold) & 0.019568 & 0.001218 & 28.3543 & 0.65123 \\
\bottomrule
\end{tabular}
\caption{General results for the \housing dataset. Cross-validation results are averaged over all
         partitions. \label{fig:results_housing_summary}}
\end{figure}

The training and evaluation times, the MSE and the $R^2$ score for SGD and SVR are shown
in Table~\ref{fig:results_housing_summary}. We used both the \ratiostd and \tenfold
evaluation methods to obtain these results. Even with this very small dataset, the SGD
regressor is faster than SVR and delivered slightly better results.

Overall, we obtained an $R^2$ score close to 70\% without performing any preprocessing
other than feature scaling.

\end{document}
