\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{xspace}

\lstset{
    language=Python,
    basicstyle=\ttfamily,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\title{Exercise 1 \\
       Machine Learning WS 2014/2015 \\
       Technical University of Vienna}
\author{Jakob Gruber, 0203440 \\
        Mino Sharkhawy, 1025887}

\newcommand{\annealing}{\texttt{annealing}\xspace}
\newcommand{\powerconsumption}{\texttt{power\_consumption}\xspace}
\newcommand{\solarflares}{\texttt{solar\_flares}\xspace}
\newcommand{\twitter}{\texttt{twitter}\xspace}

\newcommand{\tenfold}{\texttt{10fold}\xspace}
\newcommand{\ratiostd}{\texttt{ratio75}\xspace}

\begin{document}

\maketitle

\section{Introduction}

This assignment consists of:

\begin{itemize}
\item picking a good, diverse selection of datasets,
\item choosing several suitable classifier and regression techniques,
\item analyzing the behavior of the latter when run on the former,
\item while experimenting with different preprocessing techniques,
\item and finally reporting on the results (you are feasting your eyes on this
      artifact right now).
\end{itemize}

\section{Dataset Selection}

When selecting our datasets, we selected mainly for diversity in dataset features:
if one dataset had few instances, we preferred sets with a huge number of instances
as the second selection; if one set had only categorical attributes, we tried to
find a second set with integer and/or real attributes, etc. Finally, we also
preferred datasets that simply sounded interesting.

Our classification datasets are \annealing, a small dataset with a medium
number of multivariate attributes and partially missing values which ties various
attributes of the steel annealing process to the produced class of steel;
and \twitter, a huge dataset of tweets in which we interpreted words (or combinations of words)
as binary features. The purpose of the \twitter dataset is to predict the sentiment
of a tweet, i.e. if the tweet carries a positive or negative emotion.
Full disclosure: we have previously experimented on this dataset
in the \emph{Advanced Internet Computing} lecture.

Our regression datasets are \solarflares, with a medium number of both
instances and categorical features, and predicts the number of solar flares in
three potential classes. Finally, the \powerconsumption dataset contains
a huge number of instances, and a low number of  attributes (with missing values).
Some of the attributes are time-based, leading to interesting choices in preprocessing,
while the others are real values.

For further dataset details, see Figure \ref{fig:datasets}.

\begin{comment}
solar flares:
* interesting since regression on categorical data

powerconsumption:
* coupled attributes -> reduce only to time. choice of preprocessing important.
\end{comment}


\begin{figure}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\multirow{2}[4]{*}{Datasets} & \multicolumn{2}{c}{Classification} & \multicolumn{2}{c}{Regression} \\
\cmidrule(l){2-3} \cmidrule(l){4-5}
                & \twitter     & \annealing     & \solarflares   & \powerconsumption \\
\midrule
Nr. of samples  & Huge         & Medium         & Medium         & Huge              \\
Nr. of features & Huge         & Medium         & Low            & Low               \\
Feature types   & Binary       & Cat, Int, Real & Categorical    & Date, Real        \\
Missing values  & No           & Yes            & No             & Yes               \\
Preprocessing   & Beneficial   & TODO           & TODO           & TODO              \\
Result set      & \{Pos, Neg\} & 6 classes      & $\text{Int}^3$ & $\text{Real}^3$   \\
\bottomrule
\end{tabular}
\caption{Dataset details. Feature types are abbreviated as: Bin = binary, Cat = categorical, Int = integral. \label{fig:datasets}}
\end{figure}

\section{Classifier Selection}

% TODO

\section{Tools}

We based our experiments on the machine learning toolkit
\verb|scikit-learn 0.15.2|\footnote{\url{http://scikit-learn.org/stable/}}
in conjunction with the natural language toolkit
\verb|nltk 3.0a4|\footnote{\url{http://www.nltk.org/}}.

The entire application logic
(loading, preprocessing, training, and evaluation) is written in Python, giving us
maximal flexibility.

Results were exported to CSV files, which were then used to generate graphs
using R and the \verb|ggplot2|\footnote{\url{http://ggplot2.org/}} package.

Obviously, this report was typeset in \LaTeX (and less obviously, integrated with
R using \verb|sweave|\footnote{\url{https://www.stat.uni-muenchen.de/~leisch/Sweave/}}).

\section{Preprocessing} % Maybe we can handle this in results?

% TODO

\section{Results}

% TODO: Describe our general toolchain and how to reproduce our experiments.

\subsection{\twitter}

% TODO
% TODO Define \tenfold and \ratiostd.

\subsection{\annealing}

Evaluation of the \annealing dataset yielded very good results in all circumstances, even
(and surprisingly, \emph{especially}) without preprocessing.

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Classifier & Train. Time (s) & Eval. Time (s) & Accuracy \\
\midrule
Bayes (\ratiostd) & 0.0077 & 0.026 & 0.95 \\
SVM (\ratiostd)   & 0.0224 & 0.024 & 1.0  \\
Bayes (\tenfold) & 0.0091 & 0.011 & 0.96 \\
SVM (\tenfold)   & 0.0263 & 0.01 & 1.0  \\
\bottomrule
\end{tabular}
\caption{General results for the \annealing dataset. Cross-validation results are averaged over all
         partitions. \label{fig:results_annealing_summary}}
\end{figure}

Figure \ref{fig:results_annealing_summary} gives an overview of % TODO.

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Class & Class Size & Precision & Recall \\
\midrule
1 & 0 & N/A & N/A \\
2 & 23 & 1.0 & 0.78 \\
3 & 155 & 0.97 & 0.96 \\
4 & 0 & N/A & N/A \\
5 & 17 & 1.0 & 1.0 \\
U & 5 & 0.45 & 1.0 \\
\bottomrule
\end{tabular}
\caption{Precision and recall results for the \annealing dataset, using the Bayes classifier and
         \ratiostd partitioning. \label{fig:results_annealing_per_class}}
\end{figure}

\subsection{\powerconsumption}

% TODO

\subsection{\solarflares}

% TODO

\end{document}
