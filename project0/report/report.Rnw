\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{xspace}

\lstset{
    language=Python,
    basicstyle=\ttfamily,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\title{Exercise 1 \\
       Machine Learning WS 2014/2015 \\
       Technical University of Vienna}
\author{Jakob Gruber, 0203440 \\
        Mino Sharkhawy, 1025887}

\newcommand{\annealing}{\texttt{annealing}\xspace}
\newcommand{\powerconsumption}{\texttt{power\_consumption}\xspace}
\newcommand{\solarflares}{\texttt{solar\_flares}\xspace}
\newcommand{\twitter}{\texttt{twitter}\xspace}

\newcommand{\tenfold}{\texttt{10fold}\xspace}
\newcommand{\ratiostd}{\texttt{ratio75}\xspace}

\begin{document}

\maketitle

\section{Introduction}

This assignment consists of:

\begin{itemize}
\item picking a good, diverse selection of datasets,
\item choosing several suitable classifier and regression techniques,
\item analyzing the behavior of the latter when run on the former,
\item while experimenting with different preprocessing techniques,
\item and finally reporting on the results (you are feasting your eyes on this
      artifact right now).
\end{itemize}

\section{Dataset Selection}

When selecting our datasets, we selected mainly for diversity in dataset features:
if one dataset had few instances, we preferred sets with a huge number of instances
as the second selection; if one set had only categorical attributes, we tried to
find a second set with integer and/or real attributes, etc. Finally, we also
preferred datasets that simply sounded interesting.

Our classification datasets are \annealing, a small dataset with a medium
number of multivariate attributes and partially missing values which ties various
attributes of the steel annealing process to the produced class of steel;
and \twitter, a huge dataset of tweets in which we interpreted words (or combinations of words)
as binary features. The purpose of the \twitter dataset is to predict the sentiment
of a tweet, i.e. if the tweet carries a positive or negative emotion.
Full disclosure: we have previously experimented on this dataset
in the \emph{Advanced Internet Computing} lecture.

Our regression datasets are \solarflares, with a medium number of both
instances and categorical features, and predicts the number of solar flares in
three potential classes. Finally, the \powerconsumption dataset contains
a huge number of instances, and a low number of  attributes (with missing values).
Some of the attributes are time-based, leading to interesting choices in preprocessing,
while the others are real values.

For further dataset details, see Figure \ref{fig:datasets}.

\begin{comment}
solar flares:
* interesting since regression on categorical data

powerconsumption:
* coupled attributes -> reduce only to time. choice of preprocessing important.
\end{comment}


\begin{figure}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\multirow{2}[4]{*}{Datasets} & \multicolumn{2}{c}{Classification} & \multicolumn{2}{c}{Regression} \\
\cmidrule(l){2-3} \cmidrule(l){4-5}
                & \twitter     & \annealing     & \solarflares   & \powerconsumption \\
\midrule
Nr. of samples  & Huge         & Medium         & Medium         & Huge              \\
Nr. of features & Huge         & Medium         & Low            & Low               \\
Feature types   & Binary       & Cat, Int, Real & Categorical    & Date, Real        \\
Missing values  & No           & Yes            & No             & Yes               \\
Preprocessing   & Beneficial   & TODO           & TODO           & TODO              \\
Result set      & \{Pos, Neg\} & 6 classes      & $\text{Int}^3$ & $\text{Real}^3$   \\
\bottomrule
\end{tabular}
\caption{Dataset details. Feature types are abbreviated as: Bin = binary, Cat = categorical, Int = integral. \label{fig:datasets}}
\end{figure}

\section{Classifier Selection}

% TODO

\section{Tools}

We based our experiments on the machine learning toolkit
\verb|scikit-learn 0.15.2|\footnote{\url{http://scikit-learn.org/stable/}}
in conjunction with the natural language toolkit
\verb|nltk 3.0a4|\footnote{\url{http://www.nltk.org/}}.

The entire application logic
(loading, preprocessing, training, and evaluation) is written in Python, giving us
maximal flexibility.

Results were exported to CSV files, which were then used to generate graphs
using R and the \verb|ggplot2|\footnote{\url{http://ggplot2.org/}} package.

Obviously, this report was typeset in \LaTeX (and less obviously, integrated with
R using \verb|sweave|\footnote{\url{https://www.stat.uni-muenchen.de/~leisch/Sweave/}}).

\section{Preprocessing} % Maybe we can handle this in results?

% TODO

\section{Results}

% TODO: Describe our general toolchain and how to reproduce our experiments.

\subsection{\twitter}

% TODO
% TODO Define \tenfold and \ratiostd.

\subsection{\annealing}

Evaluation of the \annealing dataset yielded very good results in all circumstances,
even without preprocessing.

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Classifier & Train. Time (s) & Eval. Time (s) & Accuracy \\
\midrule
Bayes (\ratiostd) & 0.0077 & 0.026 & 0.95 \\
SVM (\ratiostd)   & 0.0224 & 0.024 & 1.0  \\
KNN (\ratiostd)   & 0.016  & 0.143 & 0.97 \\
Bayes (\tenfold)  & 0.0091 & 0.011 & 0.96 \\
SVM (\tenfold)    & 0.0263 & 0.01  & 1.0  \\
KNN (\tenfold)    & 0.0182 & 0.058 & 0.97 \\
\bottomrule
\end{tabular}
\caption{General results for the \annealing dataset without preprocessing. All instance attributes are
         treated as strings. Cross-validation results are averaged over all
         partitions. \label{fig:results_annealing_summary}}
\end{figure}

Our main results were obtained entirely
without preprocessing: all instance attributes (even real-valued ones such as length or thickness)
were treated as strings, and missing values were simply omitted. Conversion of real-valued attributes
to \lstinline|float| yielded no change in the Bayes classifier (since Bayes does not
distinguish between continuous and categorical data), and resulted in a degradation of accuracy of
between 10\% (KNN) and 20\% (SVM). Subsequent normalization of large real-valued attributes such as
length and width negated this effect, and caused the accuracy of both KNN and SVM classifiers to
return to their previous state as shown in Figure \ref{fig:results_annealing_summary}. Replacing missing
values by a dedicated value resulted in a degradation of 3\% for the Bayes classifier, and negligible
changes for SVM and KNN. We thus decided
to perform the remaining experiments without preprocessing, since results seemed optimal and the pipeline
was simpler.

Figure \ref{fig:results_annealing_summary} gives an overview of classifier performance and accuracy
for both a 75-25\% split between training and evaluation data, and using 10-fold cross-validation.
Accuracy is surprisingly high for in all cases, with all classifiers predicting at least 95\% of all
evaluation instances correctly. The SVM classifier in particular predicts optimally without making a
single mistake.

Furthermore, it stands out, that:

\begin{itemize}
\item the Bayes classifier is time-efficient both in training and classification,
\item and that the SVM is very time-intensive.
\end{itemize}

\begin{figure}[ht]
\centering
\begin{tabular}{ccc}
\toprule
Bayes & SVM & KNN \\
\midrule
24576 & 69632 & 1011712 \\
\bottomrule
\end{tabular}
\caption{Trained classifier sizes in bytes for the \annealing dataset.
         \label{fig:results_annealing_sizes}}
\end{figure}

We attempted to determine the space required by the different classifiers by using the python serialization
module \lstinline|cPickle|, pickling the trained classifiers, and finally determining the produced file's
size. The results are displayed in Figure \ref{fig:results_annealing_sizes}. In this case, it is especially
notable that the KNN classifier is larger than the others by over a factor of $14$. It is also somewhat
surprising that SVM is larger than Bayes, since we clearly obtained the opposite result on the \twitter
dataset, in which SVM offered a significant size advantage. Figure \ref{fig:results_annealing_split_size}
reveals that the size of the KNN classifier grows linearly with the number of training instances, while
SVM and Bayes have much lower growth rates.

\begin{figure}[ht]
\centering
\begin{tabular}{lccc}
\toprule
Class & Class Size & Precision & Recall \\
\midrule
1 & 0 & N/A & N/A \\
2 & 23 & 1.0 & 0.78 \\
3 & 155 & 0.97 & 0.96 \\
4 & 0 & N/A & N/A \\
5 & 17 & 1.0 & 1.0 \\
U & 5 & 0.45 & 1.0 \\
\bottomrule
\end{tabular}
\caption{Precision and recall results for the \annealing dataset, using the Bayes classifier and
         \ratiostd partitioning. Class sizes are given as the number of instances of a particular class
         within the evaluation set. \label{fig:results_annealing_per_class}}
\end{figure}

Figure \ref{fig:results_annealing_per_class} shows precision and recall statistics for a particular
execution of the Bayes classifier. Class 3 is the most common by far, and is predicted very accurately.
Classes 1 and 4 do not appear at all in this selection of evaluation instances, and the smallest class
U exhibits poor precision, i.e. over half of the instances classified as U are predicted incorrectly.

Finally, since all classifiers were overwhelmingly positive, we experimented with altering the ratio
of training to evaluation instances. Figures \ref{fig:results_annealing_split_accuracy} and
\ref{fig:results_annealing_split_size} show, respectively, the produced accuracy
and classifier sizes of the SVM, KNN, and Bayes classifiers given a certain training-evaluation ratio
ranging from 5\% to 95\%. SVM is the clear winner in this dataset, rising to 98\% accuracy for
training-evaluation ratios as low as 15\% (i.e. training on around 120 instances). SVM reaches
100\% accuracy at a ratio of 37\%, while both Bayes and KNN keep improving slowly with rising
ratios.

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_accuracy("../results/annealing_bayes_ratiorange.csv",
                          "../results/annealing_svm_ratiorange.csv",
                          "../results/annealing_knn_ratiorange.csv")
@
\caption{Classifier accuracy for varying training-evaluation ratios.
         \label{fig:results_annealing_split_accuracy}}
\end{figure}

\begin{figure}[ht]
\centering
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
plot_ratio_range_size("../results/annealing_bayes_ratiorange.csv",
                      "../results/annealing_svm_ratiorange.csv",
                      "../results/annealing_knn_ratiorange.csv")
@
\caption{Classifier sizes for varying training-evaluation ratios.
         \label{fig:results_annealing_split_size}}
\end{figure}

The \annealing dataset is obviously very well suited to classification using machine learning and produces
exceptional accuracy even with a low number of training instances. The data is skewed heavily towards
a single class, with 75\% of all instances belonging to class 3 (and, for example, none to class 4).
We assume that this is a large part of the reason why our classifiers perform so well; in fact, a naive classifier
that simply predicts class 3 every time would also have 75\% accuracy. It was especially instructive
to see the different behaviors of the classifiers both in training and evaluation times, and in
the size of the produced files. Normalization of real-valued attributes also had a larger than expected
effect on the classifier accuracy.

\subsection{\powerconsumption}

% TODO

\subsection{\solarflares}

% TODO

\end{document}
